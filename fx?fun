import pandas as pd
from pyspark.sql import DataFrame as pyspark_df
from pyspark.sql.functions import col, regexp_replace, when

class DLakeUtils: # Simulación de la clase para que el código sea ejecutable
    def __init__(self, spark):
        self.spark = spark
        self.SparkPartitions = 200 # Valor de ejemplo

    def write_log(self, message, level="INFO"):
        """Función de registro de logs simulada."""
        print(f"[{level}] {message}")

    def DLake_Replace(self,
                      temp_view_or_df,
                      dlake_tbl: str,
                      *argsv,
                      debug_schema: bool = True):
        """
        Inserta o sobreescribe datos en una tabla Hive existente, manejando tablas particionadas.
        Adapta automáticamente el esquema y soporta particiones,
        aplicando un 'safe cast' para evitar el error de convertir
        strings no numéricos a float/double/decimal.

        Args:
            temp_view_or_df: Nombre de vista temporal (str) o DataFrame de Spark.
            dlake_tbl      : Tabla destino en formato "db.tabla".
            *argsv         : Si se pasa cualquier argumento, usará parquet insertInto sin overwrite.
            debug_schema   : Si True, imprime esquemas antes de insertar.
        """
        # 1) Obtener DataFrame de entrada
        if isinstance(temp_view_or_df, str):
            df = self.spark.table(temp_view_or_df)
        elif isinstance(temp_view_or_df, pyspark_df):
            df = temp_view_or_df
        else:
            raise TypeError("temp_view_or_df debe ser nombre de vista o un Spark DataFrame.")
        if df is None:
            raise ValueError(f"El DataFrame para {temp_view_or_df} es None.")

        # 2) Validar si tiene datos
        count_df = df.count()
        if count_df == 0:
            self.write_log("! Advertencia: El DataFrame está vacío. No se realizará el INSERT.", "WARNING")
            return
        else:
            self.write_log(f"El DataFrame tiene {count_df} filas. Procediendo con {dlake_tbl}.", "INFO")

        # 3) Leer esquema de Hive y separar columnas de datos y partición
        # La salida de DESCRIBE para tablas particionadas es:
        # columnas_de_datos
        #
        # # Partition Information
        # # col_name            data_type           comment
        # columnas_de_particion
        schema_df = self.spark.sql(f"DESCRIBE {dlake_tbl}").toPandas()

        data_columns_info = []
        partition_columns_info = []
        past_data_columns = False # Bandera para saber si ya pasamos las columnas de datos

        for index, row in schema_df.iterrows():
            col_name = str(row["col_name"]).strip()
            data_type = str(row["data_type"]).strip()

            # Saltar líneas de comentario o encabezados de sección
            if col_name.startswith("#"):
                if "Partition Information" in col_name:
                    past_data_columns = True # Hemos encontrado la sección de información de partición
                continue

            # Saltar la línea en blanco que separa las columnas de datos de las de partición
            if col_name == "":
                past_data_columns = True # Después de una línea en blanco, estamos en la sección de partición
                continue

            # Clasificar la columna
            if past_data_columns:
                partition_columns_info.append((col_name, data_type))
            else:
                data_columns_info.append((col_name, data_type))

        hive_data_schema = pd.DataFrame(data_columns_info, columns=["col_name", "data_type"])
        hive_partition_schema = pd.DataFrame(partition_columns_info, columns=["col_name", "data_type"])

        # Unir los esquemas para obtener el orden total de las columnas en la tabla destino
        full_hive_schema = pd.concat([hive_data_schema, hive_partition_schema], ignore_index=True)

        # Debug de conteo de columnas
        if debug_schema:
            print(f"DEBUG: Número de columnas en DataFrame de entrada (df): {len(df.columns)}")
            print(f"DEBUG: Número de columnas de datos en tabla destino: {len(hive_data_schema)}")
            print(f"DEBUG: Número de columnas de partición en tabla destino: {len(hive_partition_schema)}")
            print(f"DEBUG: Número TOTAL de columnas esperadas en tabla destino (datos + partición): {len(full_hive_schema)}")
            print("-" * 50) # Separador para claridad

        # 4) Preparar 'safe cast' y alias de columnas para TODAS las columnas en el orden de Hive
        df_cols = {c.lower().split('.')[-1]: c for c in df.columns} # Mapeo de nombres de columnas del DF de entrada
        casted = [] # Lista para almacenar las expresiones de columna casteadas
        patron_decimal = r"^-?\d+(\.\d+)?$" # Patrón para validar números decimales
        numeric_pref = ("float", "double", "decimal") # Prefijos de tipos numéricos

        # Iterar sobre el esquema completo de la tabla destino (datos + partición)
        for name, dtype in full_hive_schema.itertuples(index=False):
            key = name.lower()
            if key not in df_cols:
                raise ValueError(f"Columna '{name}' (del esquema destino) no encontrada en el DataFrame de entrada: {df.columns}. Asegúrate de que el DF de entrada contenga todas las columnas de la tabla destino, incluyendo las de partición.")

            spark_col = col(df_cols[key]) # Columna original del DataFrame de entrada

            # Aplicar 'safe cast' solo a columnas de datos que sean numéricas
            if name in hive_data_schema["col_name"].values and any(dtype.lower().startswith(p) for p in numeric_pref):
                clean_col = regexp_replace(spark_col, r"[^0-9\.-]", "") # Limpiar caracteres no numéricos
                casted_col = when(
                    clean_col.rlike(patron_decimal), # Si cumple el patrón numérico
                    clean_col.cast(dtype)           # Castear al tipo destino
                ).otherwise(None).alias(name)       # Si no, establecer a None y renombrar
            else:
                # Para columnas de datos no numéricas o cualquier columna de partición,
                # simplemente castear directamente al tipo destino.
                casted_col = spark_col.cast(dtype).alias(name)
            casted.append(casted_col)

        # 5) Seleccionar, castear y coalesce
        # df2 ahora contendrá todas las columnas (datos + partición) en el orden y tipo correctos
        df2 = df.select(*casted).coalesce(self.SparkPartitions)

        # 6) Debug de esquemas opcional
        if debug_schema:
            print("=== Esquema Hive destino (Columnas de Datos) ===")
            print(hive_data_schema.to_string(index=False))
            print("\n=== Esquema Hive destino (Columnas de Partición) ===")
            print(hive_partition_schema.to_string(index=False))
            print("\n=== Esquema DataFrame casteado (todas las columnas, listo para insertar) ===")
            df2.printSchema()
            print()

        # 7) Vista temporal y ejecución
        tmp = "_tmp_replace"
        df2.createOrReplaceTempView(tmp)

        try:
            if len(argsv) > 0:
                # Usar insertInto para anexar datos (overwrite=False)
                # Spark manejará las particiones automáticamente si df2 contiene las columnas de partición
                df2.write.mode("overwrite").format("parquet") \
                   .insertInto(dlake_tbl, overwrite=False) # 'overwrite=False' significa APPEND
                self.write_log(f"✅ Datos anexados en {dlake_tbl} ({count_df} registros).", "INFO")
            else:
                # Usar INSERT OVERWRITE TABLE para sobreescribir datos
                # Spark manejará las particiones automáticamente si SELECT * de tmp
                # incluye las columnas de partición y coinciden con la tabla destino.
                sql = f"INSERT OVERWRITE TABLE {dlake_tbl} SELECT * FROM {tmp}"
                self.write_log(f"Ejecutando SQL:\n{sql}", "INFO")
                self.spark.sql(sql)
                self.write_log(f"✅ Datos insertados en {dlake_tbl} ({count_df} registros).", "INFO")
        except Exception as e:
            msg = f"Error en DLake_Replace para {dlake_tbl}: {e}"
            self.write_log(msg, "ERROR")
            raise


def consulta_tablas(conex, start_date, end_date, diaria, mensual, campofecha):
    """
    Placeholder para la función consulta_tablas.
    Esta función debería contener la lógica para consultar y procesar los datos
    para el rango de fechas dado y luego usar DLake_Replace para insertarlos.
    """
    print(f"  [consulta_tablas] Procesando desde {start_date} hasta {end_date} para {mensual}")
    # Aquí iría tu lógica real de consulta y procesamiento
    # Por ejemplo:
    # df_daily = conex.spark.table(diaria).filter(f"{campofecha} BETWEEN '{start_date}' AND '{end_date}'")
    # conex.DLake_Replace(df_daily, mensual)


def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max: # Asegurarse de que r_max no sea None (tabla vacía)
            max_processed_date = datetime.strptime(r_max, '%Y-%m-%d').date() if isinstance(r_max, str) else r_max
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener la fecha máxima procesada o la tabla está vacía. Error: {e}", "WARNING")
        # Si no hay datos en la tabla mensual, tomamos el mes antepasado como punto de partida.
        # Si se desea procesar desde el inicio de fechas_iniciales, se puede cambiar.
        if fechas_iniciales:
            # Establecer max_processed_date a un día antes del primer mes para que se procese
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            # Default si no hay fechas_iniciales, usar el mes antepasado
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")




def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes
                formats_to_try = ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d']
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: {r_max}")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str o date.", "WARNING")
                # Fallback a default si el tipo es inesperado
                if fechas_iniciales:
                    max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales:
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")


def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes
                formats_to_try = ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d']
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: {r_max}")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str o date.", "WARNING")
                # Fallback a default si el tipo es inesperado
                if fechas_iniciales:
                    max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales:
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es el mes en curso, no hacemos nada
    if month_to_process_start and \
       month_to_process_start.year == hoy.year and \
       month_to_process_start.month == hoy.month:
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es el mes en curso. No se hace nada.")
        return

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")


        def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, datetime): # Si es un objeto datetime (timestamp de Spark)
                max_processed_date = r_max.date()
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes, incluyendo con tiempo
                formats_to_try = [
                    '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
                    '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
                    '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
                ]
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' (tipo: {type(r_max)}) no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: '{r_max}'")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str, date o datetime.", "WARNING")
                # Fallback a default si el tipo es inesperado
                if fechas_iniciales:
                    max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales:
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es el mes en curso, no hacemos nada
    if month_to_process_start and \
       month_to_process_start.year == hoy.year and \
       month_to_process_start.month == hoy.month:
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es el mes en curso. No se hace nada.")
        return

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")



def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, datetime): # Si es un objeto datetime (timestamp de Spark)
                max_processed_date = r_max.date()
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes, incluyendo con tiempo
                formats_to_try = [
                    '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
                    '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
                    '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
                ]
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' (tipo: {type(r_max)}) no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: '{r_max}'")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str, date o datetime.", "WARNING")
                # Fallback a default si el tipo es inesperado
                if fechas_iniciales:
                    max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales:
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es el mes en curso, no hacemos nada
    if month_to_process_start and \
       month_to_process_start.year == hoy.year and \
       month_to_process_start.month == hoy.month:
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es el mes en curso. No se hace nada.")
        return

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date, manejando varios formatos
        parsed_start_date = None
        parsed_end_date = None

        # Formatos de fecha comunes (incluyendo con tiempo, por si acaso)
        formats_to_try = [
            '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
            '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
        ]

        if isinstance(start_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_start_date = datetime.strptime(start_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_start_date:
                conex.write_log(f"Error: La fecha inicial '{start_date_candidate}' no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha inicial no reconocido: '{start_date_candidate}'")
        elif isinstance(start_date_candidate, datetime):
            parsed_start_date = start_date_candidate.date()
        elif isinstance(start_date_candidate, date):
            parsed_start_date = start_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha inicial: {type(start_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha inicial no soportado: {type(start_date_candidate)}")


        if isinstance(end_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_end_date = datetime.strptime(end_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_end_date:
                conex.write_log(f"Error: La fecha final '{end_date_candidate}' no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha final no reconocido: '{end_date_candidate}'")
        elif isinstance(end_date_candidate, datetime):
            parsed_end_date = end_date_candidate.date()
        elif isinstance(end_date_candidate, date):
            parsed_end_date = end_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha final: {type(end_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha final no soportado: {type(end_date_candidate)}")


        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if parsed_start_date.month == month_to_process_start.month and \
           parsed_start_date.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {parsed_start_date} hasta {parsed_end_date}")
            consulta_tablas(conex, parsed_start_date, parsed_end_date, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")

        def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (dict): Diccionario de fechas de inicio de los rangos mensuales a considerar.
                                 Ej: {'first0mdf': datetime.date(2025,7,1), ...}
        fechas_finales (dict): Diccionario de fechas de fin de los rangos mensuales a considerar.
                               Ej: {'last0mdf': datetime.date(2025,7,31), ...}
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, datetime): # Si es un objeto datetime (timestamp de Spark)
                max_processed_date = r_max.date()
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes, incluyendo con tiempo
                formats_to_try = [
                    '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
                    '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
                    '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
                ]
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' (tipo: {type(r_max)}) no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: '{r_max}'")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str, date o datetime.", "WARNING")
                # Fallback a default si el tipo es inesperado
                # Si fechas_iniciales es un diccionario, tomamos el valor más antiguo para el fallback
                if fechas_iniciales and isinstance(fechas_iniciales, dict) and fechas_iniciales.values():
                    max_processed_date = min(fechas_iniciales.values()) - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales and isinstance(fechas_iniciales, dict) and fechas_iniciales.values():
            max_processed_date = min(fechas_iniciales.values()) - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es el mes en curso, no hacemos nada
    if month_to_process_start and \
       month_to_process_start.year == hoy.year and \
       month_to_process_start.month == hoy.month:
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es el mes en curso. No se hace nada.")
        return

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las entradas (ahora diccionarios)
    found_range_to_process = False

    # Asegurarse de que las claves de ambos diccionarios sean las mismas para un zip coherente
    if set(fechas_iniciales.keys()) != set(fechas_finales.keys()):
        conex.write_log("Error: Las claves de 'fechas_iniciales' y 'fechas_finales' no coinciden. Asegúrate de que ambos diccionarios tengan las mismas claves para emparejar las fechas correctamente.", "ERROR")
        raise ValueError("Las claves de los diccionarios de fechas no coinciden.")

    # Iterar sobre las claves para asegurar el emparejamiento correcto de fechas
    # Si el orden de las claves no es importante, se puede usar sorted(fechas_iniciales.keys())
    # Asumimos que el orden de inserción en el diccionario no es relevante,
    # y que las claves se usan para emparejar.
    for key in fechas_iniciales.keys():
        start_date_candidate = fechas_iniciales[key]
        end_date_candidate = fechas_finales[key]

        # Asegurarse de que las fechas candidatas sean objetos date, manejando varios tipos
        parsed_start_date = None
        parsed_end_date = None

        # Formatos de fecha comunes (incluyendo con tiempo, por si acaso)
        formats_to_try = [
            '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
            '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
        ]

        # Lógica para start_date_candidate
        if isinstance(start_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_start_date = datetime.strptime(start_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_start_date:
                conex.write_log(f"Error: La fecha inicial '{start_date_candidate}' (clave: '{key}') no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha inicial no reconocido para clave '{key}': '{start_date_candidate}'")
        elif isinstance(start_date_candidate, datetime):
            parsed_start_date = start_date_candidate.date()
        elif isinstance(start_date_candidate, date):
            parsed_start_date = start_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha inicial (clave: '{key}'): {type(start_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha inicial no soportado para clave '{key}': {type(start_date_candidate)}")


        # Lógica para end_date_candidate
        if isinstance(end_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_end_date = datetime.strptime(end_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_end_date:
                conex.write_log(f"Error: La fecha final '{end_date_candidate}' (clave: '{key}') no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha final no reconocido para clave '{key}': '{end_date_candidate}'")
        elif isinstance(end_date_candidate, datetime):
            parsed_end_date = end_date_candidate.date()
        elif isinstance(end_date_candidate, date):
            parsed_end_date = end_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha final (clave: '{key}'): {type(end_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha final no soportado para clave '{key}': {type(end_date_candidate)}")


        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if parsed_start_date.month == month_to_process_start.month and \
           parsed_start_date.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {parsed_start_date} hasta {parsed_end_date}")
            consulta_tablas(conex, parsed_start_date, parsed_end_date, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")





def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (dict): Diccionario de fechas de inicio de los rangos mensuales a considerar.
                                 Ej: {'first0mdf': datetime.date(2025,7,1), 'first1mdf': datetime.date(2025,6,1), ...}
        fechas_finales (dict): Diccionario de fechas de fin de los rangos mensuales a considerar.
                               Ej: {'last0mdf': datetime.date(2025,7,31), 'last1mdf': datetime.date(2025,6,30), ...}
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, datetime): # Si es un objeto datetime (timestamp de Spark)
                max_processed_date = r_max.date()
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes, incluyendo con tiempo
                formats_to_try = [
                    '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
                    '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
                    '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
                ]
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' (tipo: {type(r_max)}) no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: '{r_max}'")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str, date o datetime.", "WARNING")
                # Fallback a default si el tipo es inesperado
                # Si fechas_iniciales es un diccionario, tomamos el valor más antiguo para el fallback
                if fechas_iniciales and isinstance(fechas_iniciales, dict) and fechas_iniciales.values():
                    # Asegurarse de que los valores sean parseables a date antes de min()
                    valid_dates = []
                    for val in fechas_iniciales.values():
                        if isinstance(val, date):
                            valid_dates.append(val)
                        elif isinstance(val, datetime):
                            valid_dates.append(val.date())
                        elif isinstance(val, str):
                            for fmt in formats_to_try:
                                try:
                                    parsed_val = datetime.strptime(val, fmt).date()
                                    valid_dates.append(parsed_val)
                                    break
                                except ValueError:
                                    continue
                    if valid_dates:
                        max_processed_date = min(valid_dates) - relativedelta(days=1)
                    else:
                        max_processed_date = hoy - relativedelta(months=2)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales and isinstance(fechas_iniciales, dict) and fechas_iniciales.values():
            valid_dates = []
            for val in fechas_iniciales.values():
                if isinstance(val, date):
                    valid_dates.append(val)
                elif isinstance(val, datetime):
                    valid_dates.append(val.date())
                elif isinstance(val, str):
                    for fmt in formats_to_try:
                        try:
                            parsed_val = datetime.strptime(val, fmt).date()
                            valid_dates.append(parsed_val)
                            break
                        except ValueError:
                            continue
            if valid_dates:
                max_processed_date = min(valid_dates) - relativedelta(days=1)
            else:
                max_processed_date = hoy - relativedelta(months=2)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es el mes en curso, no hacemos nada
    if month_to_process_start and \
       month_to_process_start.year == hoy.year and \
       month_to_process_start.month == hoy.month:
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es el mes en curso. No se hace nada.")
        return

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las entradas (diccionarios con claves dinámicas)
    found_range_to_process = False

    # Obtener las claves de fechas_iniciales y ordenarlas para un procesamiento consistente.
    # Asumimos que la parte numérica de la clave (ej. '0mdf', '1mdf') es lo que permite el orden.
    sorted_initial_keys = sorted(fechas_iniciales.keys())

    for initial_key in sorted_initial_keys:
        # Derivar la clave correspondiente para fechas_finales
        # Esto asume que la clave de fechas_finales es la misma que la de fechas_iniciales
        # pero con 'first' reemplazado por 'last'.
        expected_final_key = initial_key.replace('first', 'last')

        # Verificar que la clave derivada exista en fechas_finales
        if expected_final_key not in fechas_finales:
            conex.write_log(f"Error: La clave esperada '{expected_final_key}' para la fecha final (derivada de '{initial_key}') no se encontró en 'fechas_finales'. Asegúrate de que ambos diccionarios tengan las claves correspondientes.", "ERROR")
            raise ValueError(f"Clave '{expected_final_key}' faltante en fechas_finales para emparejar con '{initial_key}'.")

        start_date_candidate = fechas_iniciales[initial_key]
        end_date_candidate = fechas_finales[expected_final_key] # Usar la clave derivada para acceder


        # Asegurarse de que las fechas candidatas sean objetos date, manejando varios tipos
        parsed_start_date = None
        parsed_end_date = None

        # Formatos de fecha comunes (incluyendo con tiempo, por si acaso)
        formats_to_try = [
            '%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', # Solo fecha
            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', # Fecha y hora
            '%Y/%m/%d %H:%M:%S', '%Y/%m/%d %H:%M:%S.%f'
        ]

        # Lógica para start_date_candidate
        if isinstance(start_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_start_date = datetime.strptime(start_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_start_date:
                conex.write_log(f"Error: La fecha inicial '{start_date_candidate}' (clave: '{initial_key}') no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha inicial no reconocido para clave '{initial_key}': '{start_date_candidate}'")
        elif isinstance(start_date_candidate, datetime):
            parsed_start_date = start_date_candidate.date()
        elif isinstance(start_date_candidate, date):
            parsed_start_date = start_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha inicial (clave: '{initial_key}'): {type(start_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha inicial no soportado para clave '{initial_key}': {type(start_date_candidate)}")


        # Lógica para end_date_candidate
        if isinstance(end_date_candidate, str):
            for fmt in formats_to_try:
                try:
                    parsed_end_date = datetime.strptime(end_date_candidate, fmt).date()
                    break
                except ValueError:
                    continue
            if not parsed_end_date:
                conex.write_log(f"Error: La fecha final '{end_date_candidate}' (clave: '{expected_final_key}') no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                raise ValueError(f"Formato de fecha final no reconocido para clave '{expected_final_key}': '{end_date_candidate}'")
        elif isinstance(end_date_candidate, datetime):
            parsed_end_date = end_date_candidate.date()
        elif isinstance(end_date_candidate, date):
            parsed_end_date = end_date_candidate
        else:
            conex.write_log(f"Advertencia: Tipo inesperado para fecha final (clave: '{expected_final_key}'): {type(end_date_candidate)}. Esperado str, date o datetime.", "WARNING")
            raise TypeError(f"Tipo de fecha final no soportado para clave '{expected_final_key}': {type(end_date_candidate)}")


        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if parsed_start_date.month == month_to_process_start.month and \
           parsed_start_date.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {parsed_start_date} hasta {parsed_end_date}")
            consulta_tablas(conex, parsed_start_date, parsed_end_date, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql.functions import (
    col, lit, when, to_timestamp, to_date, hour, minute, concat_ws,
    lower, max, sum, trim, month, date_format, row_number, substring
)
from pyspark.sql.types import DateType, IntegerType, FloatType
import pyspark.sql.functions as F
from typing import Dict, Tuple

# ==============================================================================
#                               FUNCIONES AUXILIARES
# ==============================================================================

def _fill_nulls(df: DataFrame) -> DataFrame:
    """
    Rellena los valores nulos en un DataFrame con cadenas vacías para evitar errores de tipo en joins.
    """
    return df.select(*[when(col(c).isNull(), '').otherwise(col(c)).alias(c) for c in df.columns])

def _process_datetime_columns(df: DataFrame, time_col: str, alias_map: dict) -> DataFrame:
    """
    Función auxiliar para procesar las columnas de fecha y hora.
    Ajusta la zona horaria, extrae el intervalo de 30 minutos y crea la fecha.
    """
    return df.withColumn(
        time_col, to_timestamp(col(time_col) - F.expr('INTERVAL 6 HOURS'))
    ).withColumn(
        'record_date', to_date(col(time_col))
    ).withColumn(
        'interval_hour', concat_ws(
            ':',
            hour(col(time_col)),
            when(minute(col(time_col)) < 30, lit('00')).otherwise(lit('30'))
        )
    ).select(
        'interval_hour', 'record_date', *[col(c).alias(alias_map.get(c, c)) for c in df.columns if c not in [time_col]]
    )

def _process_catalogo_dialer(df_cat_dialer: DataFrame) -> Tuple[DataFrame, DataFrame]:
    """
    Procesa el catálogo del dialer para obtener las tablas de saturación y disposiciones
    que se usarán en el procesamiento de outbound.
    """
    df_cat_dialer_list = df_cat_dialer.select(
        trim(col('aspect_list_name')).alias('aspect_list_name'), col('saturacion')
    ).groupBy('aspect_list_name').agg(
        max(col('saturacion')).alias('saturacion')
    ).select('aspect_list_name', 'saturacion')

    df_cat_dialer_disp = df_cat_dialer.select(
        trim(col('disposition')).alias('disposition'),
        col('attempt'), col('penetration'), col('connection'),
        col('direct_contact'), col('promise'), col('abandon'),
        trim(col('group_disposition')).alias('group_disposition')
    )
    
    return df_cat_dialer_list, df_cat_dialer_disp

# ==============================================================================
#                          FUNCIONES DE PROCESAMIENTO
# ==============================================================================

def jerarquia_dialer(df_jerarquia_dialer: DataFrame) -> Tuple:
    """
    Procesa un DataFrame maestro de jerarquías para crear DataFrames específicos
    basados en la columna 'tab', eliminando duplicados y ordenando por fecha de procesamiento.
    """
    try:
        df_jerarquia_dialer = df_jerarquia_dialer.sort('process_date', ascending=False)

        # 1. Report Groups to Super Groups
        jerarquia_dialer_hist_rg_sg = df_jerarquia_dialer.filter(col('tab') == 'Report Groups to Super Groups')
        jerarquia_dialer_hist_rg_sg = jerarquia_dialer_hist_rg_sg.select(
            col('reportnamemasterid'), col('reportname'), col('supergroupmasterid'), col('supergroupname'),
            to_date(col('startdate'), "dd/MM/yyyy").alias('startdate_sg'),
            to_date(col('stopdate'), "dd/MM/yyyy").alias('stopdate_sg'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['reportnamemasterid', 'reportname', 'supergroupmasterid', 'supergroupname', 'startdate_sg'])
        
        # 2. LOB Group - ReportGrp (Inbound)
        jerarquia_dialer_hist_lg_rg_ib = df_jerarquia_dialer.filter(
            (col('tab') == 'LOB Group - ReportGrp') & (col('classification') == 'IB Service')
        )
        jerarquia_dialer_hist_lg_rg_ib = jerarquia_dialer_hist_lg_rg_ib.select(
            col('lobmasterid'), col('groupname'),
            to_date(col('lobtorgstartdate'), "dd/MM/yyyy").alias('lobtorgstartdate'),
            to_date(col('lobtorgstopdate'), "dd/MM/yyyy").alias('lobtorgstopdate'),
            col('reportnamemasterid'), col('reportname'),
            to_date(col('rgstartdate'), "dd/MM/yyyy").alias('rgstartdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['lobmasterid', 'groupname', 'lobtorgstartdate', 'reportnamemasterid', 'reportname'])

        # 3. LOBs to Service Ids (Inbound)
        jerarquia_dialer_hist_lob_sid = df_jerarquia_dialer.filter(
            (col('tab') == 'LOBs to Service Ids') & (col('classification') == 'IB Service')
        )
        jerarquia_dialer_hist_lob_sid = jerarquia_dialer_hist_lob_sid.select(
            col('lobmasterid'), col('groupname'),
            to_date(col('startdate'), "dd/MM/yyyy").alias('startdate_serviceid'),
            to_date(col('stopdate'), "dd/MM/yyyy").alias('stopdate_serviceid'),
            col('uip_inst'), col('uip_inst_serviceid'), col('serviceid'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['lobmasterid', 'groupname', 'startdate', 'serviceid'])

        # 4. LOB Group - ReportGrp (Outbound)
        jerarquia_dialer_hist_lg_rg_ob = df_jerarquia_dialer.filter(
            (col('tab') == 'LOB Group - ReportGrp') & (col('classification') == 'List')
        )
        jerarquia_dialer_hist_lg_rg_ob = jerarquia_dialer_hist_lg_rg_ob.select(
            col('lobmasterid'), col('groupname'),
            to_date(col('lobtorgstartdate'), "dd/MM/yyyy").alias('lobtorgstartdate'),
            to_date(col('lobtorgstopdate'), "dd/MM/yyyy").alias('lobtorgstopdate'),
            col('reportnamemasterid'), col('reportname'),
            to_date(col('rgstartdate'), "dd/MM/yyyy").alias('rgstartdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['lobmasterid', 'groupname', 'lobtorgstartdate', 'reportnamemasterid', 'reportname'])

        # 5. LOBs to ALMLists (Outbound)
        jerarquia_dialer_hist_lob_alm = df_jerarquia_dialer.filter(col('tab') == 'LOBs to ALMLists')
        jerarquia_dialer_hist_lob_alm = jerarquia_dialer_hist_lob_alm.select(
            col('lobmasterid'), col('groupname'), col('uip_inst'), col('listname'),
            to_date(col('listname_startdate'), "dd/MM/yyyy").alias('listname_startdate'),
            to_date(col('listname_stopdate'), "dd/MM/yyyy").alias('listname_stopdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['lobmasterid', 'groupname', 'uip_inst', 'listname', 'listname_startdate'])

        # 6. ALMList Active Goals (Outbound)
        jerarquia_dialer_hist_alm_active = df_jerarquia_dialer.filter(col('tab') == 'ALMList Active Goals')
        jerarquia_dialer_hist_alm_active = jerarquia_dialer_hist_alm_active.select(
            col('listname').alias('listname_active'),
            to_date(col('updatedate'), "dd/MM/yyyy").alias('updatedate_active'),
            col('goallow').alias('goallow_active'), col('goalhigh').alias('goalhigh_active'),
            to_date(col('startdate'), "dd/MM/yyyy").alias('startdate_active'),
            to_date(col('stopdate'), "dd/MM/yyyy").alias('stopdate_active'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['listname_active', 'updatedate_active', 'startdate_active'])
        
        # 7. LOB Group - ReportGrp (Staff)
        jerarquia_dialer_hist_lg_rg_staff = df_jerarquia_dialer.filter(col('tab') == 'LOB Group - ReportGrp')
        jerarquia_dialer_hist_lg_rg_staff = jerarquia_dialer_hist_lg_rg_staff.select(
            col('lobmasterid'), col('groupname'),
            to_date(col('lobtorgstartdate'), "dd/MM/yyyy").alias('lobtorgstartdate'),
            to_date(col('lobtorgstopdate'), "dd/MM/yyyy").alias('lobtorgstopdate'),
            col('reportnamemasterid'), col('reportname'),
            to_date(col('rgstartdate'), "dd/MM/yyyy").alias('rgstartdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['lobmasterid', 'groupname', 'lobtorgstartdate', 'reportnamemasterid', 'reportname'])
        
        # 8. NICEMU-WorkSeg-ReportGrp Config (Staff)
        jerarquia_dialer_hist_mu_rg = df_jerarquia_dialer.filter(col('tab') == 'NICEMU-WorkSeg-ReportGrp Config')
        jerarquia_dialer_hist_mu_rg = jerarquia_dialer_hist_mu_rg.select(
            col('reportnamemasterid'), col('reportname'), col('mu_id'), col('nicemu'),
            to_date(col('nicemu_startdate'), "dd/MM/yyyy").alias('nicemu_startdate'),
            to_date(col('nicemu_stopdate'), "dd/MM/yyyy").alias('nicemu_stopdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['reportnamemasterid', 'reportname', 'mu_id', 'nicemu', 'nicemu_startdate'])

        # 9. Forecast Group to Report Group (Staff)
        jerarquia_dialer_hist_fg_rg_staff = df_jerarquia_dialer.filter(col('tab') == 'Forecast Group to Report Group')
        jerarquia_dialer_hist_fg_rg_staff = jerarquia_dialer_hist_fg_rg_staff.select(
            col('reportnamemasterid'), col('reportname'), col('fcstgrpid'), col('forecast_group_code'),
            to_date(col('fcst_group_code_startdate'), "dd/MM/yyyy").alias('fcst_group_code_startdate'),
            to_date(col('fcst_group_code_stopdate'), "dd/MM/yyyy").alias('fcst_group_code_stopdate'),
            to_date(col('process_date'), "yyyy-MM-dd").alias('process_date')
        ).dropDuplicates(subset=['reportnamemasterid', 'reportname', 'fcstgrpid', 'forecast_group_code', 'fcst_group_code_startdate'])

        return (jerarquia_dialer_hist_rg_sg, jerarquia_dialer_hist_lg_rg_ib, jerarquia_dialer_hist_lob_sid,
                jerarquia_dialer_hist_lg_rg_ob, jerarquia_dialer_hist_lob_alm, jerarquia_dialer_hist_alm_active,
                jerarquia_dialer_hist_lg_rg_staff, jerarquia_dialer_hist_mu_rg, jerarquia_dialer_hist_fg_rg_staff)
                
    except Exception as e:
        print(f"Error en la función jerarquia_dialer: {e}")
        return None

def data_inbound(
    df_contact_event: DataFrame,
    df_int_agent_det: DataFrame,
    jerarquia_dialer_hist_fg_rg_staff: DataFrame,
    jerarquia_dialer_hist_rg_sg: DataFrame
) -> DataFrame:
    """
    Procesa los datos de inbound combinando eventos de contacto, interacciones de agentes
    y las jerarquías del dialer.
    """
    try:
        df_contact_event_proc = _process_datetime_columns(
            df_contact_event,
            'time_of_contact',
            {'contact_list_name': 'contact_list_name', 'response_status': 'response_status',
             'agent_login_name': 'agent_login_name', 'total_number_of_records': 'total_number_of_records',
             'seqnum': 'seqnum'}
        )

        df_int_agent_det_proc = _process_datetime_columns(
            df_int_agent_det.filter(col('agenttime') > 0),
            'interactionstartdt',
            {'user_id': 'user_id', 'seqnum': 'seqnum', 'agenttime': 'agenttime',
             'previewtime': 'previewtime', 'activetime': 'activetime',
             'wraptime': 'wraptime', 'holdtime': 'holdtime'}
        )
        
        join_key_contact = ['record_date', 'interval_hour', 'contact_list_name']
        contact_event_grouped = df_contact_event_proc.groupBy(*join_key_contact).agg(
            sum('total_number_of_records').alias('total_number_of_records'),
            sum(when(col('response_status') != '', 1).otherwise(0)).alias('records_with_response_status')
        )

        join_key_int_agent = ['record_date', 'interval_hour', 'user_id', 'seqnum']
        int_agent_grouped = df_int_agent_det_proc.groupBy(*join_key_int_agent).agg(
            *[sum(c).alias(c) for c in ['agenttime', 'previewtime', 'activetime', 'wraptime', 'holdtime']]
        )
        
        uip_inbound_d = contact_event_grouped.join(
            int_agent_grouped,
            on=[
                contact_event_grouped['record_date'] == int_agent_grouped['record_date'],
                contact_event_grouped['interval_hour'] == int_agent_grouped['interval_hour']
            ],
            how='full_outer'
        ).drop(int_agent_grouped['record_date']).drop(int_agent_grouped['interval_hour'])

    except Exception as e:
        print(f"Error en el procesamiento inicial de data_inbound: {e}")
        return None

    try:
        uip_inbound_d = uip_inbound_d.join(
            jerarquia_dialer_hist_fg_rg_staff,
            on=(
                (uip_inbound_d['contact_list_name'] == jerarquia_dialer_hist_fg_rg_staff['fcstgrpid']) &
                (uip_inbound_d['record_date'] >= jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_startdate']) &
                (uip_inbound_d['record_date'] <= when(
                    jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_stopdate'].isNull(),
                    uip_inbound_d['record_date']
                ).otherwise(jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_stopdate']))
            ),
            how='inner'
        ).withColumn('reportnamemasterid', jerarquia_dialer_hist_fg_rg_staff['reportnamemasterid'])\
         .withColumn('reportname', jerarquia_dialer_hist_fg_rg_staff['reportname'])

        uip_inbound_d = uip_inbound_d.join(
            jerarquia_dialer_hist_rg_sg,
            on=(
                (uip_inbound_d['reportnamemasterid'] == jerarquia_dialer_hist_rg_sg['reportnamemasterid']) &
                (uip_inbound_d['record_date'] >= jerarquia_dialer_hist_rg_sg['startdate_sg']) &
                (uip_inbound_d['record_date'] <= when(
                    jerarquia_dialer_hist_rg_sg['stopdate_sg'].isNull(),
                    uip_inbound_d['record_date']
                ).otherwise(jerarquia_dialer_hist_rg_sg['stopdate_sg']))
            ),
            how='inner'
        ).withColumn('supergroupname', jerarquia_dialer_hist_rg_sg['supergroupname'])

    except Exception as e:
        print(f"Error en las uniones con las jerarquías: {e}")
        return None

    uip_inbound_d_final = uip_inbound_d.withColumn(
        'month', month(col('record_date'))
    ).drop('reportnamemasterid', 'contact_list_name')

    columnas_finales = [
        col('interval_hour'), col('record_date').cast(DateType()), col('supergroupname'),
        col('reportname'), col('total_number_of_records').cast(IntegerType()),
        col('records_with_response_status').cast(IntegerType()), col('agenttime').cast(IntegerType()),
        col('previewtime').cast(IntegerType()), col('activetime').cast(IntegerType()),
        col('wraptime').cast(IntegerType()), col('holdtime').cast(IntegerType()),
        col('month').cast(IntegerType())
    ]
    
    uip_inbound_d_final = uip_inbound_d_final.select(*columnas_finales).sort('record_date')
    
    return _fill_nulls(uip_inbound_d_final)

def data_outbound(
    df_contact_event: DataFrame,
    df_int_agent_det: DataFrame,
    df_cat_dialer: DataFrame,
    jerarquia_dialer_hist_lob_alm: DataFrame,
    jerarquia_dialer_hist_alm_active: DataFrame,
    jerarquia_dialer_hist_lg_rg_ob: DataFrame,
    jerarquia_dialer_hist_rg_sg: DataFrame
) -> DataFrame:
    """
    Procesa los datos de outbound combinando información de eventos de contacto,
    detalle de agentes y jerarquías del dialer.
    """
    try:
        df_contact_event_proc = df_contact_event.select(
            col('contact_list_name'), col('response_status'), lower(col('agent_login_name')).alias('agent_login_name'),
            col('total_number_of_records'), col('seqnum'),
            to_timestamp(col('time_of_contact') - F.expr('INTERVAL 6 HOURS')).alias('time_of_contact')
        ).withColumn(
            'record_date', to_date(col('time_of_contact'))
        ).withColumn(
            'interval_hour', concat_ws(
                ':',
                hour(col('time_of_contact')),
                when(minute(col('time_of_contact')) < 30, lit('00')).otherwise(lit('30'))
            )
        )
        
        max_tot_num_rec = df_contact_event_proc.withColumn(
            'join_key', concat_ws('-', col('record_date'), col('contact_list_name'))
        ).groupBy('join_key').agg(
            max('total_number_of_records').alias('max_total_number_of_records')
        )

        not_emp_res_sta = df_contact_event_proc.filter(col('response_status') != '').withColumn(
            'join_key', concat_ws('-', col('record_date'), col('contact_list_name'))
        ).groupBy('join_key').agg(
            max('total_number_of_records').alias('max_total_number_of_records_not_emp_res_sta')
        )
        
        df_contact_event_final = df_contact_event_proc.withColumn(
            'join_key', concat_ws('-', col('record_date'), col('interval_hour'), col('seqnum'), col('agent_login_name'))
        ).join(
            df_contact_event_proc.withColumn('join_key', concat_ws('-', col('record_date'), col('contact_list_name')))\
                                  .join(max_tot_num_rec, on='join_key', how='left')\
                                  .join(not_emp_res_sta, on='join_key', how='left'),
            on=df_contact_event_proc['join_key'] == df_int_agent_det_proc['join_key'], # Corregir este join
            how='left'
        )

    except Exception as e:
        print(f"Error en el procesamiento de ContactEvent: {e}")
        return None

    try:
        df_int_agent_det_proc = df_int_agent_det.filter(
            col('agenttime') > 0
        ).withColumn(
            'interactionstartdt', to_timestamp(col('interactionstartdt') - F.expr('INTERVAL 6 HOURS'))
        ).withColumn(
            'record_date', to_date(col('interactionstartdt'))
        ).withColumn(
            'interval_hour', concat_ws(
                ':',
                hour(col('interactionstartdt')),
                when(minute(col('interactionstartdt')) < 30, lit('00')).otherwise(lit('30'))
            )
        ).withColumn(
            'join_key', concat_ws('-', col('record_date'), col('interval_hour'), col('seqnum'), lower(col('user_id')))
        )

        df_int_agent_det_grouped = df_int_agent_det_proc.groupBy('join_key').agg(
            sum('agenttime').alias('agenttime'),
            sum('previewtime').alias('previewtime'),
            sum('activetime').alias('activetime'),
            sum('wraptime').alias('wraptime'),
            sum('holdtime').alias('holdtime')
        )

        df_contact_event_final = df_contact_event_final.withColumn(
            'join_key', concat_ws('-', col('record_date'), col('interval_hour'), col('seqnum'), col('agent_login_name'))
        ).join(
            df_int_agent_det_grouped, on='join_key', how='left'
        ).drop('join_key')

    except Exception as e:
        print(f"Error en el procesamiento de InteractionAgentDetail o su unión: {e}")
        return None

    try:
        df_cat_dialer_list, df_cat_dialer_disp = _process_catalogo_dialer(df_cat_dialer)
        
        uip_outbound_d = df_contact_event_final.join(
            df_cat_dialer_list,
            on=df_contact_event_final['contact_list_name'] == df_cat_dialer_list['aspect_list_name'],
            how='left'
        ).join(
            df_cat_dialer_disp,
            on=df_contact_event_final['response_status'] == df_cat_dialer_disp['disposition'],
            how='left'
        )
    except Exception as e:
        print(f"Error en la unión con el catálogo del dialer: {e}")
        return None
        
    try:
        uip_outbound_d = uip_outbound_d.join(
            jerarquia_dialer_hist_lob_alm.distinct(),
            on=uip_outbound_d['contact_list_name'] == jerarquia_dialer_hist_lob_alm['listname'],
            how='left'
        ).withColumn('lobmasterid', jerarquia_dialer_hist_lob_alm['lobmasterid'])\
         .withColumn('groupname', jerarquia_dialer_hist_lob_alm['groupname'])

        uip_outbound_d = uip_outbound_d.join(
            jerarquia_dialer_hist_alm_active.select('listname_active', 'goalhigh_active').distinct(),
            on=uip_outbound_d['contact_list_name'] == jerarquia_dialer_hist_alm_active['listname_active'],
            how='left'
        ).withColumn('goalhigh_active', jerarquia_dialer_hist_alm_active['goalhigh_active'])
        
        uip_outbound_d = uip_outbound_d.join(
            jerarquia_dialer_hist_lg_rg_ob,
            on=(
                (uip_outbound_d['lobmasterid'] == jerarquia_dialer_hist_lg_rg_ob['lobmasterid']) &
                (uip_outbound_d['record_date'] >= jerarquia_dialer_hist_lg_rg_ob['lobtorgstartdate']) &
                (uip_outbound_d['record_date'] <= when(
                    jerarquia_dialer_hist_lg_rg_ob['lobtorgstopdate'].isNull(),
                    uip_outbound_d['record_date']
                ).otherwise(jerarquia_dialer_hist_lg_rg_ob['lobtorgstopdate']))
            ),
            how='left'
        ).withColumn('reportnamemasterid', jerarquia_dialer_hist_lg_rg_ob['reportnamemasterid'])\
         .withColumn('reportname', jerarquia_dialer_hist_lg_rg_ob['reportname'])
        
        uip_outbound_d = uip_outbound_d.join(
            jerarquia_dialer_hist_rg_sg,
            on=(
                (uip_outbound_d['reportnamemasterid'] == jerarquia_dialer_hist_rg_sg['reportnamemasterid']) &
                (uip_outbound_d['record_date'] >= jerarquia_dialer_hist_rg_sg['startdate_sg']) &
                (uip_outbound_d['record_date'] <= when(
                    jerarquia_dialer_hist_rg_sg['stopdate_sg'].isNull(),
                    uip_outbound_d['record_date']
                ).otherwise(jerarquia_dialer_hist_rg_sg['stopdate_sg']))
            ),
            how='left'
        ).withColumn('supergroupname', jerarquia_dialer_hist_rg_sg['supergroupname'])
        
    except Exception as e:
        print(f"Error en las uniones con la jerarquía del dialer: {e}")
        return None
        
    uip_outbound_d = uip_outbound_d.withColumn(
        'supergroupname', when(col('supergroupname').isNull(), 'Sin Asignar').otherwise(col('supergroupname'))
    ).withColumn(
        'reportname', when(col('reportname').isNull(), 'Sin Asignar').otherwise(col('reportname'))
    ).withColumn(
        'month', month(col('record_date'))
    ).drop(
        'lobmasterid', 'reportnamemasterid'
    )
    
    columnas_finales = [
        col('interval_hour'), col('record_date').cast(DateType()), col('supergroupname'), col('reportname'),
        col('contact_list_name'), col('agent_login_name'), col('max_total_number_of_records').cast(IntegerType()),
        col('max_total_number_of_records_not_emp_res_sta').cast(IntegerType()), col('agenttime').cast(IntegerType()),
        col('previewtime').cast(IntegerType()), col('activetime').cast(IntegerType()), col('wraptime').cast(IntegerType()),
        col('holdtime').cast(IntegerType()), col('saturacion').cast(IntegerType()), col('attempt').cast(IntegerType()),
        col('penetration').cast(IntegerType()), col('connection').cast(IntegerType()), col('direct_contact').cast(IntegerType()),
        col('promise').cast(IntegerType()), col('abandon').cast(IntegerType()),
        col('group_disposition'), col('goalhigh_active').cast(FloatType()), col('month').cast(IntegerType())
    ]
    
    uip_outbound_d = uip_outbound_d.select(*columnas_finales).sort('record_date')
    
    return _fill_nulls(uip_outbound_d)

def data_staff(
    df_agent_act_sum: DataFrame,
    df_int_agent_det: DataFrame,
    jerarquia_dialer_hist_rg_sg: DataFrame,
    jerarquia_dialer_hist_mu_rg: DataFrame,
    df_nice_agent_info: DataFrame
) -> DataFrame:
    """
    Procesa los datos de staff combinando información de actividad del agente,
    interacciones y jerarquías.
    """
    try:
        df_agent_act_sum_proc = df_agent_act_sum.filter(col('service_id') == 0).select(
            lower(col('user_id')).alias('user_id'), col('service_id'),
            col('totallogintime'), col('totalidletime'), col('totalnotreadytime'),
            col('totalgaptime'), col('totalparkidletime'), col('totalparktime'),
            to_timestamp(col('begintimeperioddt') - F.expr('INTERVAL 6 HOURS')).alias('begintimeperioddt')
        ).withColumn(
            'record_date', to_date(col('begintimeperioddt'))
        ).withColumn(
            'interval_hour', concat_ws(':', hour(col('begintimeperioddt')), when(minute(col('begintimeperioddt')) < 30, lit('00')).otherwise(lit('30')))
        )
        
        df_agent_act_sum_grouped = df_agent_act_sum_proc.groupBy(
            'interval_hour', 'record_date', 'user_id', 'service_id'
        ).agg(
            *[sum(c).alias(c) for c in [
                'totallogintime', 'totalidletime', 'totalnotreadytime',
                'totalgaptime', 'totalparkidletime', 'totalparktime'
            ]]
        )

        df_int_agent_det_proc = df_int_agent_det.filter(col('agenttime') > 0).select(
            lower(col('user_id')).alias('user_id'), col('seqnum'),
            col('agenttime'), col('previewtime'), col('activetime'),
            col('wraptime'), col('holdtime'), 
            to_timestamp(col('interactionstartdt') - F.expr('INTERVAL 6 HOURS')).alias('interactionstartdt')
        ).withColumn(
            'record_date', to_date(col('interactionstartdt'))
        ).withColumn(
            'interval_hour', concat_ws(':', hour(col('interactionstartdt')), when(minute(col('interactionstartdt')) < 30, lit('00')).otherwise(lit('30')))
        )
        
        df_int_agent_det_grouped = df_int_agent_det_proc.groupBy(
            'interval_hour', 'record_date', 'user_id'
        ).agg(
            *[sum(c).alias(c) for c in [
                'agenttime', 'activetime', 'wraptime', 'previewtime', 'holdtime'
            ]]
        )
        
        join_key_cols = ['record_date', 'interval_hour', 'user_id']
        uip_staff_d = df_agent_act_sum_grouped.join(
            df_int_agent_det_grouped, on=join_key_cols, how='left'
        )
    except Exception as e:
        print(f"Error en el procesamiento de AgentActivity o InteractionDetail: {e}")
        return None
    
    try:
        window_spec = Window.partitionBy('date', 'muid', 'externalid').orderBy(col('process_date').desc())
        df_nice_agent_info_proc = df_nice_agent_info.withColumn(
            'date_nice_agent_info', date_format(to_date(col('date'), 'yyyyMMdd'), 'yyyy-MM-dd')
        ).withColumn(
            'agent_info_id', lower(col('externalid'))
        ).withColumn(
            'process_date', to_date(col('process_date'))
        ).withColumn(
            'row_num', row_number().over(window_spec)
        ).filter(
            col('row_num') == 1
        ).drop('row_num', 'date', 'externalid')
        
        uip_staff_d = uip_staff_d.join(
            df_nice_agent_info_proc,
            on=[
                uip_staff_d['record_date'] == df_nice_agent_info_proc['date_nice_agent_info'],
                uip_staff_d['user_id'] == df_nice_agent_info_proc['agent_info_id']
            ],
            how='left'
        ).select(
            uip_staff_d['*'],
            df_nice_agent_info_proc['muid'].alias('nice_muid'),
            df_nice_agent_info_proc['agent_info_id'].alias('nice_externalid')
        ).drop('date_nice_agent_info', 'agent_info_id', 'process_date')

    except Exception as e:
        print(f"Error en la unión con df_nice_agent_info: {e}")
        return None

    try:
        uip_staff_d = uip_staff_d.join(
            jerarquia_dialer_hist_mu_rg,
            on=(
                (uip_staff_d['nice_muid'] == jerarquia_dialer_hist_mu_rg['mu_id']) &
                (uip_staff_d['record_date'] >= jerarquia_dialer_hist_mu_rg['nicemu_startdate']) &
                (uip_staff_d['record_date'] <= when(
                    jerarquia_dialer_hist_mu_rg['nicemu_stopdate'].isNull(),
                    uip_staff_d['record_date']
                ).otherwise(jerarquia_dialer_hist_mu_rg['nicemu_stopdate']))
            ),
            how='left'
        ).withColumn('reportnamemasterid', jerarquia_dialer_hist_mu_rg['reportnamemasterid'])\
         .withColumn('reportname', jerarquia_dialer_hist_mu_rg['reportname'])
        
        uip_staff_d = uip_staff_d.join(
            jerarquia_dialer_hist_rg_sg,
            on=(
                (uip_staff_d['reportnamemasterid'] == jerarquia_dialer_hist_rg_sg['reportnamemasterid']) &
                (uip_staff_d['record_date'] >= jerarquia_dialer_hist_rg_sg['startdate_sg']) &
                (uip_staff_d['record_date'] <= when(
                    jerarquia_dialer_hist_rg_sg['stopdate_sg'].isNull(),
                    uip_staff_d['record_date']
                ).otherwise(jerarquia_dialer_hist_rg_sg['stopdate_sg']))
            ),
            how='left'
        ).withColumn('supergroupname', jerarquia_dialer_hist_rg_sg['supergroupname'])
        
    except Exception as e:
        print(f"Error en las uniones con las jerarquías: {e}")
        return None

    uip_staff_d_final = uip_staff_d.withColumn(
        'supergroupname', when(col('supergroupname').isNull(), 'Sin Asignar').otherwise(col('supergroupname'))
    ).withColumn(
        'reportname', when(col('reportname').isNull(), 'Sin Asignar').otherwise(col('reportname'))
    ).withColumn(
        'month', month(col('record_date'))
    ).drop(
        'nice_muid', 'nice_externalid', 'reportnamemasterid', 'service_id'
    )
    
    columnas_finales = [
        col('interval_hour'), col('record_date').cast(DateType()), col('supergroupname'), col('reportname'),
        col('user_id'), col('totallogintime').cast(IntegerType()),
        col('totalidletime').cast(IntegerType()), col('totalnotreadytime').cast(IntegerType()),
        col('totalgaptime').cast(IntegerType()), col('totalparkidletime').cast(IntegerType()),
        col('totalparktime').cast(IntegerType()), col('agenttime').cast(IntegerType()),
        col('previewtime').cast(IntegerType()), col('activetime').cast(IntegerType()),
        col('wraptime').cast(IntegerType()), col('holdtime').cast(IntegerType()), col('month').cast(IntegerType())
    ]
    
    uip_staff_d_final = uip_staff_d_final.select(*columnas_finales).sort('record_date')
    
    return _fill_nulls(uip_staff_d_final)

def data_forecast(
    jerarquia_dialer_hist_rg_sg: DataFrame,
    jerarquia_dialer_hist_fg_rg_staff: DataFrame,
    df_nice_active_forecast: DataFrame
) -> DataFrame:
    """
    Procesa los datos de previsión combinando la previsión activa de Nice
    con las jerarquías del dialer.
    """
    try:
        df_nice_active_forecast_proc = df_nice_active_forecast.withColumn(
            'date_nice_active_fcst', date_format(to_date(col('date'), 'yyyyMMdd'), 'yyyy-MM-dd')
        ).withColumn(
            'period_hour', when(hour(to_timestamp(col('period'), 'H:mm')) < 10, substring(col('period'), 1, 1)).otherwise(substring(col('period'), 1, 2))
        ).withColumn(
            'period_minute', substring(col('period'), -2, 2)
        ).withColumn(
            'period_nice_active_fcst', concat_ws(':', col('period_hour'), col('period_minute'))
        ).withColumn(
            'process_date', to_date(col('process_date'))
        )
        
        window_spec = Window.partitionBy('date_nice_active_fcst', 'period_nice_active_fcst', 'ctid').orderBy(col('process_date').desc())
        df_nice_active_forecast_final = df_nice_active_forecast_proc.withColumn('row_num', row_number().over(window_spec))\
            .filter(col('row_num') == 1)\
            .drop('row_num', 'period_hour', 'period_minute', 'process_date', 'date', 'period')

    except Exception as e:
        print(f"Error en el procesamiento inicial de df_nice_active_forecast: {e}")
        return None

    try:
        uip_nice_active_forecast_d = df_nice_active_forecast_final.join(
            jerarquia_dialer_hist_fg_rg_staff,
            on=(
                (df_nice_active_forecast_final['ctid'] == jerarquia_dialer_hist_fg_rg_staff['fcstgrpid']) &
                (df_nice_active_forecast_final['date_nice_active_fcst'] >= jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_startdate']) &
                (df_nice_active_forecast_final['date_nice_active_fcst'] <= when(
                    jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_stopdate'].isNull(),
                    df_nice_active_forecast_final['date_nice_active_fcst']
                ).otherwise(jerarquia_dialer_hist_fg_rg_staff['fcst_group_code_stopdate']))
            ),
            how='inner'
        ).withColumn('reportnamemasterid', jerarquia_dialer_hist_fg_rg_staff['reportnamemasterid'])\
         .withColumn('reportname', jerarquia_dialer_hist_fg_rg_staff['reportname'])

        uip_nice_active_forecast_d = uip_nice_active_forecast_d.join(
            jerarquia_dialer_hist_rg_sg,
            on=(
                (uip_nice_active_forecast_d['reportnamemasterid'] == jerarquia_dialer_hist_rg_sg['reportnamemasterid']) &
                (uip_nice_active_forecast_d['date_nice_active_fcst'] >= jerarquia_dialer_hist_rg_sg['startdate_sg']) &
                (uip_nice_active_forecast_d['date_nice_active_fcst'] <= when(
                    jerarquia_dialer_hist_rg_sg['stopdate_sg'].isNull(),
                    uip_nice_active_forecast_d['date_nice_active_fcst']
                ).otherwise(jerarquia_dialer_hist_rg_sg['stopdate_sg']))
            ),
            how='inner'
        ).withColumn('supergroupname', jerarquia_dialer_hist_rg_sg['supergroupname'])
        
    except Exception as e:
        print(f"Error en las uniones con las jerarquías: {e}")
        return None

    uip_nice_active_forecast_d_final = uip_nice_active_forecast_d.withColumn(
        'month', month(col('date_nice_active_fcst'))
    ).drop('ctid', 'reportnamemasterid')
    
    columnas_finales = [
        col('period_nice_active_fcst'), col('date_nice_active_fcst').cast(DateType()),
        col('supergroupname'), col('reportname'), col('fcstcontactsreceived').cast(FloatType()),
        col('fcstaht').cast(FloatType()), col('fcstreq').cast(FloatType()),
        col('schedopen').cast(FloatType()), col('month').cast(IntegerType())
    ]
    
    uip_nice_active_forecast_d_final = uip_nice_active_forecast_d_final.select(*columnas_finales).sort('date_nice_active_fcst')

    return _fill_nulls(uip_nice_active_forecast_d_final)

def data_nice_agent_adherence_summary(
    df_nice_agent_adherence_summary: DataFrame,
    jerarquia_dialer_hist_mu_rg: DataFrame,
    jerarquia_dialer_hist_rg_sg: DataFrame
) -> DataFrame:
    """
    Procesa los datos de adherencia del agente combinando el resumen de Nice
    con las jerarquías del dialer.
    """
    try:
        window_spec = Window.partitionBy(
            'date_nice_agent_adh_summary', 'muid', 'attribute', 'externalid'
        ).orderBy(col('process_date').desc())

        df_nice_agent_adh_proc = df_nice_agent_adherence_summary.withColumn(
            'date_nice_agent_adh_summary', date_format(to_date(col('date'), 'yyyyMMdd'), 'yyyy-MM-dd')
        ).withColumn(
            'attribute', trim(col('attribute'))
        ).withColumn(
            'total_active', (col('totalact') / 3600).cast(FloatType())
        ).withColumn(
            'total_scheduled', (col('totalsched') / 3600).cast(FloatType())
        ).withColumn(
            'unitmanagerid', lower(substring(col('unitmanager'), -7, 7))
        ).withColumn(
            'logonid', lower(col('logonid'))
        ).withColumn(
            'process_date', to_date(col('process_date'))
        ).withColumn(
            'row_num', row_number().over(window_spec)
        ).filter(
            col('row_num') == 1
        ).drop(
            'row_num', 'process_date', 'date'
        )

    except Exception as e:
        print(f"Error en el procesamiento inicial de df_nice_agent_adherence_summary: {e}")
        return None
    
    try:
        uip_nice_agent_adh_summary_d = df_nice_agent_adh_proc.join(
            jerarquia_dialer_hist_mu_rg,
            on=(
                (df_nice_agent_adh_proc['muid'] == jerarquia_dialer_hist_mu_rg['mu_id']) &
                (df_nice_agent_adh_proc['date_nice_agent_adh_summary'] >= jerarquia_dialer_hist_mu_rg['nicemu_startdate']) &
                (df_nice_agent_adh_proc['date_nice_agent_adh_summary'] <= when(
                    jerarquia_dialer_hist_mu_rg['nicemu_stopdate'].isNull(),
                    df_nice_agent_adh_proc['date_nice_agent_adh_summary']
                ).otherwise(jerarquia_dialer_hist_mu_rg['nicemu_stopdate']))
            ),
            how='inner'
        ).withColumn('reportnamemasterid', jerarquia_dialer_hist_mu_rg['reportnamemasterid'])\
         .withColumn('reportname', jerarquia_dialer_hist_mu_rg['reportname'])

        uip_nice_agent_adh_summary_d = uip_nice_agent_adh_summary_d.join(
            jerarquia_dialer_hist_rg_sg,
            on=(
                (uip_nice_agent_adh_summary_d['reportnamemasterid'] == jerarquia_dialer_hist_rg_sg['reportnamemasterid']) &
                (uip_nice_agent_adh_summary_d['date_nice_agent_adh_summary'] >= jerarquia_dialer_hist_rg_sg['startdate_sg']) &
                (uip_nice_agent_adh_summary_d['date_nice_agent_adh_summary'] <= when(
                    jerarquia_dialer_hist_rg_sg['stopdate_sg'].isNull(),
                    uip_nice_agent_adh_summary_d['date_nice_agent_adh_summary']
                ).otherwise(jerarquia_dialer_hist_rg_sg['stopdate_sg']))
            ),
            how='inner'
        ).withColumn('supergroupname', jerarquia_dialer_hist_rg_sg['supergroupname'])

    except Exception as e:
        print(f"Error en las uniones con las jerarquías: {e}")
        return None

    uip_nice_agent_adh_summary_d_final = uip_nice_agent_adh_summary_d.withColumn(
        'month', month(col('date_nice_agent_adh_summary'))
    ).drop('muid', 'reportnamemasterid')

    columnas_finales = [
        col('date_nice_agent_adh_summary').cast(DateType()), col('supergroupname'),
        col('reportname'), col('unitmanager'), col('unitmanagerid'),
        col('logonid'), col('attribute'), col('total_active').cast(FloatType()),
        col('total_scheduled').cast(FloatType()), col('month').cast(IntegerType())
    ]

    uip_nice_agent_adh_summary_d_final = uip_nice_agent_adh_summary_d_final.select(*columnas_finales).sort('date_nice_agent_adh_summary')
    
    return _fill_nulls(uip_nice_agent_adh_summary_d_final)

def nice_dialer(
    df_nice_agent_info: DataFrame,
    df_nice_agent_dialer: DataFrame,
    df_nice_agent_hier_master: DataFrame
) -> DataFrame:
    """
    Procesa las jerarquías de Nice para generar una tabla maestra de agentes con sus IDs
    y jerarquías asociadas, extrayendo el registro más reciente.
    """
    try:
        window_spec_info = Window.partitionBy('date', 'muid', 'externalid').orderBy(col('process_date').desc())
        df_nice_agent_info_proc = df_nice_agent_info.withColumn(
            'row_num', row_number().over(window_spec_info)
        ).filter(
            col('row_num') == 1
        ).select(
            col('muid').alias('muid_info'),
            lower(col('externalid')).alias('externalid_info'),
            to_date(col('date'), 'yyyyMMdd').alias('date_info')
        ).drop('row_num')

        window_spec_dialer = Window.partitionBy('date', 'loginid', 'listname').orderBy(col('process_date').desc())
        df_nice_agent_dialer_proc = df_nice_agent_dialer.withColumn(
            'row_num', row_number().over(window_spec_dialer)
        ).filter(
            col('row_num') == 1
        ).select(
            to_date(col('date'), 'yyyyMMdd').alias('date_dialer'),
            lower(col('loginid')).alias('loginid_dialer'),
            trim(col('listname')).alias('listname_dialer')
        ).drop('row_num')

        window_spec_hier = Window.partitionBy('externalid', 'subgroupname').orderBy(col('process_date').desc())
        df_nice_agent_hier_proc = df_nice_agent_hier_master.withColumn(
            'row_num', row_number().over(window_spec_hier)
        ).filter(
            col('row_num') == 1
        ).select(
            lower(col('externalid')).alias('externalid_hier'),
            trim(col('subgroupname')).alias('subgroupname_hier'),
            trim(col('supervisorname')).alias('supervisorname_hier'),
            trim(col('reportname')).alias('reportname_hier'),
            trim(col('lobname')).alias('lobname_hier'),
            trim(col('campaignname')).alias('campaignname_hier')
        ).drop('row_num')

        df_unificado = df_nice_agent_info_proc.join(
            df_nice_agent_dialer_proc,
            on=[
                df_nice_agent_info_proc['date_info'] == df_nice_agent_dialer_proc['date_dialer'],
                df_nice_agent_info_proc['externalid_info'] == df_nice_agent_dialer_proc['loginid_dialer']
            ],
            how='left'
        )

        df_unificado = df_unificado.join(
            df_nice_agent_hier_proc,
            on=df_unificado['externalid_info'] == df_nice_agent_hier_proc['externalid_hier'],
            how='left'
        )

        columnas_finales = [
            col('date_info').alias('date'),
            col('muid_info').alias('muid'),
            col('externalid_info').alias('externalid'),
            col('listname_dialer').alias('listname'),
            col('subgroupname_hier').alias('subgroupname'),
            col('supervisorname_hier').alias('supervisorname'),
            col('reportname_hier').alias('reportname'),
            col('lobname_hier').alias('lobname'),
            col('campaignname_hier').alias('campaignname')
        ]

        df_final = df_unificado.select(*columnas_finales)

        return _fill_nulls(df_final)

    except Exception as e:
        print(f"Error en el procesamiento de nice_dialer: {e}")
        return None
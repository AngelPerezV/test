import pandas as pd
from pyspark.sql import DataFrame as pyspark_df
from pyspark.sql.functions import col, regexp_replace, when

class DLakeUtils: # Simulación de la clase para que el código sea ejecutable
    def __init__(self, spark):
        self.spark = spark
        self.SparkPartitions = 200 # Valor de ejemplo

    def write_log(self, message, level="INFO"):
        """Función de registro de logs simulada."""
        print(f"[{level}] {message}")

    def DLake_Replace(self,
                      temp_view_or_df,
                      dlake_tbl: str,
                      *argsv,
                      debug_schema: bool = True):
        """
        Inserta o sobreescribe datos en una tabla Hive existente, manejando tablas particionadas.
        Adapta automáticamente el esquema y soporta particiones,
        aplicando un 'safe cast' para evitar el error de convertir
        strings no numéricos a float/double/decimal.

        Args:
            temp_view_or_df: Nombre de vista temporal (str) o DataFrame de Spark.
            dlake_tbl      : Tabla destino en formato "db.tabla".
            *argsv         : Si se pasa cualquier argumento, usará parquet insertInto sin overwrite.
            debug_schema   : Si True, imprime esquemas antes de insertar.
        """
        # 1) Obtener DataFrame de entrada
        if isinstance(temp_view_or_df, str):
            df = self.spark.table(temp_view_or_df)
        elif isinstance(temp_view_or_df, pyspark_df):
            df = temp_view_or_df
        else:
            raise TypeError("temp_view_or_df debe ser nombre de vista o un Spark DataFrame.")
        if df is None:
            raise ValueError(f"El DataFrame para {temp_view_or_df} es None.")

        # 2) Validar si tiene datos
        count_df = df.count()
        if count_df == 0:
            self.write_log("! Advertencia: El DataFrame está vacío. No se realizará el INSERT.", "WARNING")
            return
        else:
            self.write_log(f"El DataFrame tiene {count_df} filas. Procediendo con {dlake_tbl}.", "INFO")

        # 3) Leer esquema de Hive y separar columnas de datos y partición
        # La salida de DESCRIBE para tablas particionadas es:
        # columnas_de_datos
        #
        # # Partition Information
        # # col_name            data_type           comment
        # columnas_de_particion
        schema_df = self.spark.sql(f"DESCRIBE {dlake_tbl}").toPandas()

        data_columns_info = []
        partition_columns_info = []
        past_data_columns = False # Bandera para saber si ya pasamos las columnas de datos

        for index, row in schema_df.iterrows():
            col_name = str(row["col_name"]).strip()
            data_type = str(row["data_type"]).strip()

            # Saltar líneas de comentario o encabezados de sección
            if col_name.startswith("#"):
                if "Partition Information" in col_name:
                    past_data_columns = True # Hemos encontrado la sección de información de partición
                continue

            # Saltar la línea en blanco que separa las columnas de datos de las de partición
            if col_name == "":
                past_data_columns = True # Después de una línea en blanco, estamos en la sección de partición
                continue

            # Clasificar la columna
            if past_data_columns:
                partition_columns_info.append((col_name, data_type))
            else:
                data_columns_info.append((col_name, data_type))

        hive_data_schema = pd.DataFrame(data_columns_info, columns=["col_name", "data_type"])
        hive_partition_schema = pd.DataFrame(partition_columns_info, columns=["col_name", "data_type"])

        # Unir los esquemas para obtener el orden total de las columnas en la tabla destino
        full_hive_schema = pd.concat([hive_data_schema, hive_partition_schema], ignore_index=True)

        # Debug de conteo de columnas
        if debug_schema:
            print(f"DEBUG: Número de columnas en DataFrame de entrada (df): {len(df.columns)}")
            print(f"DEBUG: Número de columnas de datos en tabla destino: {len(hive_data_schema)}")
            print(f"DEBUG: Número de columnas de partición en tabla destino: {len(hive_partition_schema)}")
            print(f"DEBUG: Número TOTAL de columnas esperadas en tabla destino (datos + partición): {len(full_hive_schema)}")
            print("-" * 50) # Separador para claridad

        # 4) Preparar 'safe cast' y alias de columnas para TODAS las columnas en el orden de Hive
        df_cols = {c.lower().split('.')[-1]: c for c in df.columns} # Mapeo de nombres de columnas del DF de entrada
        casted = [] # Lista para almacenar las expresiones de columna casteadas
        patron_decimal = r"^-?\d+(\.\d+)?$" # Patrón para validar números decimales
        numeric_pref = ("float", "double", "decimal") # Prefijos de tipos numéricos

        # Iterar sobre el esquema completo de la tabla destino (datos + partición)
        for name, dtype in full_hive_schema.itertuples(index=False):
            key = name.lower()
            if key not in df_cols:
                raise ValueError(f"Columna '{name}' (del esquema destino) no encontrada en el DataFrame de entrada: {df.columns}. Asegúrate de que el DF de entrada contenga todas las columnas de la tabla destino, incluyendo las de partición.")

            spark_col = col(df_cols[key]) # Columna original del DataFrame de entrada

            # Aplicar 'safe cast' solo a columnas de datos que sean numéricas
            if name in hive_data_schema["col_name"].values and any(dtype.lower().startswith(p) for p in numeric_pref):
                clean_col = regexp_replace(spark_col, r"[^0-9\.-]", "") # Limpiar caracteres no numéricos
                casted_col = when(
                    clean_col.rlike(patron_decimal), # Si cumple el patrón numérico
                    clean_col.cast(dtype)           # Castear al tipo destino
                ).otherwise(None).alias(name)       # Si no, establecer a None y renombrar
            else:
                # Para columnas de datos no numéricas o cualquier columna de partición,
                # simplemente castear directamente al tipo destino.
                casted_col = spark_col.cast(dtype).alias(name)
            casted.append(casted_col)

        # 5) Seleccionar, castear y coalesce
        # df2 ahora contendrá todas las columnas (datos + partición) en el orden y tipo correctos
        df2 = df.select(*casted).coalesce(self.SparkPartitions)

        # 6) Debug de esquemas opcional
        if debug_schema:
            print("=== Esquema Hive destino (Columnas de Datos) ===")
            print(hive_data_schema.to_string(index=False))
            print("\n=== Esquema Hive destino (Columnas de Partición) ===")
            print(hive_partition_schema.to_string(index=False))
            print("\n=== Esquema DataFrame casteado (todas las columnas, listo para insertar) ===")
            df2.printSchema()
            print()

        # 7) Vista temporal y ejecución
        tmp = "_tmp_replace"
        df2.createOrReplaceTempView(tmp)

        try:
            if len(argsv) > 0:
                # Usar insertInto para anexar datos (overwrite=False)
                # Spark manejará las particiones automáticamente si df2 contiene las columnas de partición
                df2.write.mode("overwrite").format("parquet") \
                   .insertInto(dlake_tbl, overwrite=False) # 'overwrite=False' significa APPEND
                self.write_log(f"✅ Datos anexados en {dlake_tbl} ({count_df} registros).", "INFO")
            else:
                # Usar INSERT OVERWRITE TABLE para sobreescribir datos
                # Spark manejará las particiones automáticamente si SELECT * de tmp
                # incluye las columnas de partición y coinciden con la tabla destino.
                sql = f"INSERT OVERWRITE TABLE {dlake_tbl} SELECT * FROM {tmp}"
                self.write_log(f"Ejecutando SQL:\n{sql}", "INFO")
                self.spark.sql(sql)
                self.write_log(f"✅ Datos insertados en {dlake_tbl} ({count_df} registros).", "INFO")
        except Exception as e:
            msg = f"Error en DLake_Replace para {dlake_tbl}: {e}"
            self.write_log(msg, "ERROR")
            raise


def consulta_tablas(conex, start_date, end_date, diaria, mensual, campofecha):
    """
    Placeholder para la función consulta_tablas.
    Esta función debería contener la lógica para consultar y procesar los datos
    para el rango de fechas dado y luego usar DLake_Replace para insertarlos.
    """
    print(f"  [consulta_tablas] Procesando desde {start_date} hasta {end_date} para {mensual}")
    # Aquí iría tu lógica real de consulta y procesamiento
    # Por ejemplo:
    # df_daily = conex.spark.table(diaria).filter(f"{campofecha} BETWEEN '{start_date}' AND '{end_date}'")
    # conex.DLake_Replace(df_daily, mensual)


def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max: # Asegurarse de que r_max no sea None (tabla vacía)
            max_processed_date = datetime.strptime(r_max, '%Y-%m-%d').date() if isinstance(r_max, str) else r_max
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener la fecha máxima procesada o la tabla está vacía. Error: {e}", "WARNING")
        # Si no hay datos en la tabla mensual, tomamos el mes antepasado como punto de partida.
        # Si se desea procesar desde el inicio de fechas_iniciales, se puede cambiar.
        if fechas_iniciales:
            # Establecer max_processed_date a un día antes del primer mes para que se procese
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            # Default si no hay fechas_iniciales, usar el mes antepasado
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")




def Historica(conex, fechas_iniciales, fechas_finales, diaria, mensual, campofecha, processdate):
    """
    Gestiona la ingesta de datos históricos mensuales, asegurando que un mes
    completo no se re-ingeste si ya ha sido procesado.

    Args:
        conex: Objeto de conexión a Spark (ej. instancia de DLakeUtils).
        fechas_iniciales (list[date]): Lista de fechas de inicio de los rangos mensuales a considerar.
        fechas_finales (list[date]): Lista de fechas de fin de los rangos mensuales a considerar.
        diaria (str): Nombre de la tabla diaria de donde se extraen los datos.
        mensual (str): Nombre de la tabla mensual donde se insertan los datos.
        campofecha (str): Nombre de la columna de fecha en la tabla diaria.
        processdate (str): Nombre de la columna de fecha de procesamiento en la tabla mensual.
    """
    hoy = datetime.today().date()  # Fecha de hoy

    # 1. Obtener la fecha máxima ya procesada en la tabla mensual
    rangos_max = conex.spark.table(mensual).selectExpr(f"max({processdate}) as max_date").collect()

    max_processed_date = None
    try:
        r_max = rangos_max[0][0]
        if r_max:
            if isinstance(r_max, date): # Si ya es un objeto date (ej. de tipo DATE de Spark)
                max_processed_date = r_max
            elif isinstance(r_max, str): # Si es una cadena, intentar parsear
                # Intentar formatos de fecha comunes
                formats_to_try = ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d']
                parsed = False
                for fmt in formats_to_try:
                    try:
                        max_processed_date = datetime.strptime(r_max, fmt).date()
                        parsed = True
                        break
                    except ValueError:
                        continue
                if not parsed:
                    conex.write_log(f"Error: La fecha máxima procesada '{r_max}' no coincide con los formatos esperados ({', '.join(formats_to_try)}).", "ERROR")
                    raise ValueError(f"Formato de fecha de processdate no reconocido: {r_max}")
            else:
                conex.write_log(f"Advertencia: Tipo inesperado para max_date: {type(r_max)}. Esperado str o date.", "WARNING")
                # Fallback a default si el tipo es inesperado
                if fechas_iniciales:
                    max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
                else:
                    max_processed_date = hoy - relativedelta(months=2)
    except Exception as e:
        conex.write_log(f"Advertencia: No se pudo obtener o parsear la fecha máxima procesada. Error: {e}", "WARNING")
        # Lógica de fallback si ocurre algún error durante la extracción o el parseo inicial
        if fechas_iniciales:
            max_processed_date = fechas_iniciales[0] - relativedelta(days=1)
        else:
            max_processed_date = hoy - relativedelta(months=2)


    print("📌 Fecha máxima registrada (processdate):", max_processed_date)
    print("📆 Hoy:", hoy)

    # 2. Determinar el mes que necesita ser procesado
    month_to_process_start = None
    if max_processed_date:
        last_day_of_max_month = date(max_processed_date.year, max_processed_date.month,
                                     calendar.monthrange(max_processed_date.year, max_processed_date.month)[1])

        if max_processed_date == last_day_of_max_month:
            # Si la fecha máxima procesada es el último día de su mes, el mes ya está completo.
            # Pasamos al primer día del mes siguiente.
            month_to_process_start = last_day_of_max_month + relativedelta(days=1)
            print(f"✅ Mes {max_processed_date.strftime('%Y-%m')} ya procesado. Considerando el siguiente mes.")
        else:
            # Si la fecha máxima procesada NO es el último día de su mes, ese mes está incompleto.
            # Queremos completar ese mes, así que el mes a procesar es el de max_processed_date (primer día del mes).
            month_to_process_start = date(max_processed_date.year, max_processed_date.month, 1)
            print(f"⚠️ Mes {max_processed_date.strftime('%Y-%m')} incompleto. Procesando para completarlo.")
    else:
        # Si no hay fecha máxima procesada (tabla mensual vacía),
        # tomamos el mes antepasado como punto de partida para la ingesta.
        month_to_process_start = hoy - relativedelta(months=2)
        month_to_process_start = date(month_to_process_start.year, month_to_process_start.month, 1)
        print(f"ℹ️ Tabla mensual vacía. Iniciando ingesta desde {month_to_process_start.strftime('%Y-%m')}.")

    # Si el mes a procesar es futuro, no hacemos nada
    if month_to_process_start and (month_to_process_start.year > hoy.year or \
                                   (month_to_process_start.year == hoy.year and month_to_process_start.month > hoy.month)):
        print(f"❌ El mes a procesar ({month_to_process_start.strftime('%Y-%m')}) es futuro. No se hace nada.")
        return

    # 3. Verificar si estamos en el rango de ejecución permitido (días 7-15)
    # Esta es una regla de negocio que se mantiene.
    if not (hoy.day >= 7 and hoy.day <= 15):
        print(f"⏳ No estás en rango de ejecución mensual (día {hoy.day}). Se ejecuta solo del día 7 al 15 del mes.")
        return

    print("✅ Ejecutando ingesta de historia mensual...")

    # 4. Encontrar el rango de fechas adecuado de las listas de entrada
    found_range_to_process = False
    for start_date_candidate, end_date_candidate in zip(fechas_iniciales, fechas_finales):
        # Asegurarse de que las fechas candidatas sean objetos date
        if isinstance(start_date_candidate, str):
            start_date_candidate = datetime.strptime(start_date_candidate, '%Y-%m-%d').date()
        if isinstance(end_date_candidate, str):
            end_date_candidate = datetime.strptime(end_date_candidate, '%Y-%m-%d').date()

        # Si el mes y año de la fecha de inicio candidata coinciden con el mes a procesar
        if start_date_candidate.month == month_to_process_start.month and \
           start_date_candidate.year == month_to_process_start.year:
            print(f"⏳ Procesando rango: {start_date_candidate} hasta {end_date_candidate}")
            consulta_tablas(conex, start_date_candidate, end_date_candidate, diaria, mensual, campofecha)
            found_range_to_process = True
            break # Una vez que encontramos y procesamos el mes, salimos del bucle

    if not found_range_to_process:
        print(f"❌ No se encontró un rango de fechas iniciales/finales para el mes {month_to_process_start.strftime('%Y-%m')}.")
